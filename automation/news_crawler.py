#!/usr/bin/env python3
"""
ë¸”ë¡œê·¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
- RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
- AIë¡œ ìë™ ìš”ì•½
- data.json ìë™ ì—…ë°ì´íŠ¸
"""

import feedparser
import json
import os
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import time


class NewsAutomation:
    def __init__(self, config_path="config.json"):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.openai_api_key = os.getenv('OPENAI_API_KEY', self.config.get('openai_api_key', ''))
        
    def fetch_rss_feeds(self) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        articles = []
        
        for feed_info in self.config['rss_feeds']:
            print(f"ğŸ“¡ {feed_info['name']} í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
            
            try:
                feed = feedparser.parse(feed_info['url'])
                
                for entry in feed.entries[:feed_info.get('max_items', 3)]:
                    article = {
                        'title': entry.get('title', 'ì œëª© ì—†ìŒ'),
                        'source': feed_info['name'],
                        'link': entry.get('link', '#'),
                        'published': entry.get('published', ''),
                        'summary': entry.get('summary', entry.get('description', '')),
                        'image': self._extract_image(entry)
                    }
                    articles.append(article)
                    
                print(f"  âœ… {len(feed.entries[:feed_info.get('max_items', 3)])}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
        return articles
    
    def _extract_image(self, entry) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        # 1. media:content íƒœê·¸ì—ì„œ ì°¾ê¸°
        if hasattr(entry, 'media_content'):
            for media in entry.media_content:
                if 'url' in media:
                    return media['url']
        
        # 2. enclosuresì—ì„œ ì°¾ê¸°
        if hasattr(entry, 'enclosures'):
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image/'):
                    return enc.get('href', '')
        
        # 3. summaryì—ì„œ img íƒœê·¸ ì°¾ê¸°
        if hasattr(entry, 'summary'):
            soup = BeautifulSoup(entry.summary, 'html.parser')
            img = soup.find('img')
            if img and img.get('src'):
                return img['src']
        
        # 4. ê¸°ë³¸ ì´ë¯¸ì§€ (Unsplash random)
        return "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&w=800&q=80"
    
    def summarize_with_ai(self, article: Dict) -> Dict:
        """AIë¡œ ê¸°ì‚¬ ìš”ì•½ (OpenAI API ì‚¬ìš©)"""
        if not self.openai_api_key:
            print("  âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ì›ë³¸ ìš”ì•½ ì‚¬ìš©")
            article['summary'] = self._clean_html(article['summary'])[:200] + "..."
            return article
        
        try:
            # HTML íƒœê·¸ ì œê±°
            clean_summary = self._clean_html(article['summary'])
            
            # API ìš”ì²­
            headers = {
                'Authorization': f'Bearer {self.openai_api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'gpt-3.5-turbo',
                'messages': [
                    {'role': 'system', 'content': 'ë‹¹ì‹ ì€ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”.'},
                    {'role': 'user', 'content': f"ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\nì œëª©: {article['title']}\n\në‚´ìš©: {clean_summary[:1000]}"}
                ],
                'max_tokens': 150,
                'temperature': 0.7
            }
            
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                article['summary'] = result['choices'][0]['message']['content'].strip()
                print(f"  ğŸ¤– AI ìš”ì•½ ì™„ë£Œ: {article['title'][:30]}...")
            else:
                print(f"  âš ï¸ API ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code}), ì›ë³¸ ì‚¬ìš©")
                article['summary'] = clean_summary[:200] + "..."
                
        except Exception as e:
            print(f"  âŒ AI ìš”ì•½ ì‹¤íŒ¨: {e}, ì›ë³¸ ì‚¬ìš©")
            article['summary'] = self._clean_html(article['summary'])[:200] + "..."
        
        return article
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        soup = BeautifulSoup(text, 'html.parser')
        return soup.get_text(strip=True)
    
    def calculate_time_ago(self, published_str: str) -> str:
        """ê²Œì‹œ ì‹œê°„ì„ 'ëª‡ ì‹œê°„ ì „' í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            from dateutil import parser
            pub_date = parser.parse(published_str)
            now = datetime.now(pub_date.tzinfo)
            diff = now - pub_date
            
            if diff.days > 0:
                return f"{diff.days}ì¼ ì „"
            elif diff.seconds >= 3600:
                return f"{diff.seconds // 3600}ì‹œê°„ ì „"
            elif diff.seconds >= 60:
                return f"{diff.seconds // 60}ë¶„ ì „"
            else:
                return "ë°©ê¸ˆ ì „"
        except:
            return "ìµœê·¼"
    
    def generate_data_json(self, articles: List[Dict]) -> Dict:
        """data.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        processed_articles = []
        
        for article in articles[:self.config.get('max_articles', 20)]:
            # AI ìš”ì•½ ì ìš©
            if self.config.get('use_ai_summary', False):
                article = self.summarize_with_ai(article)
                time.sleep(1)  # API ìš”ì²­ ê°„ê²©
            else:
                article['summary'] = self._clean_html(article['summary'])[:200] + "..."
            
            processed_articles.append({
                'title': article['title'],
                'source': article['source'],
                'time': self.calculate_time_ago(article.get('published', '')),
                'summary': article['summary'],
                'link': article['link'],
                'image': article['image']
            })
        
        return {
            'updatedAt': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'articles': processed_articles
        }
    
    def save_data_json(self, data: Dict, output_path='data.json'):
        """data.json ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… {output_path} ì €ì¥ ì™„ë£Œ! ({len(data['articles'])}ê°œ ê¸°ì‚¬)")
    
    def run(self):
        """ì „ì²´ ìë™í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=" * 50)
        print("ğŸš€ ë¸”ë¡œê·¸ ìë™í™” ì‹œì‘")
        print("=" * 50)
        
        # 1. RSS í”¼ë“œ ìˆ˜ì§‘
        print("\n[1ë‹¨ê³„] RSS í”¼ë“œ ìˆ˜ì§‘")
        articles = self.fetch_rss_feeds()
        
        if not articles:
            print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")
        
        # 2. ë°ì´í„° ê°€ê³µ & AI ìš”ì•½
        print("\n[2ë‹¨ê³„] ë°ì´í„° ê°€ê³µ ë° AI ìš”ì•½")
        data = self.generate_data_json(articles)
        
        # 3. data.json ì €ì¥
        print("\n[3ë‹¨ê³„] íŒŒì¼ ì €ì¥")
        self.save_data_json(data)
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ìë™í™” ì™„ë£Œ!")
        print("=" * 50)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    automation = NewsAutomation()
    automation.run()


if __name__ == "__main__":
    main()
