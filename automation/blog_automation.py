#!/usr/bin/env python3
"""
í†µí•© ë¸”ë¡œê·¸ ìë™í™” ì‹œìŠ¤í…œ
- RSS ë‰´ìŠ¤ í¬ë¡¤ë§ + AI ì½˜í…ì¸  ìƒì„±
- data.json í†µí•© ì—…ë°ì´íŠ¸
"""

import json
import os
import sys
from datetime import datetime
from typing import List, Dict

# ê¸°ì¡´ ëª¨ë“ˆ
from news_crawler import NewsAutomation

# ìƒˆ ëª¨ë“ˆ
try:
    from ai_content_generator import AIContentGenerator
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    print("âš ï¸ AI ì½˜í…ì¸  ìƒì„± ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë ¤ë©´ 'pip install google-generativeai' ì‹¤í–‰")


class BlogAutomation:
    def __init__(self, 
                 rss_config='config.json',
                 ai_config='config_ai.json',
                 enable_ai=True):
        """
        í†µí•© ë¸”ë¡œê·¸ ìë™í™” ì´ˆê¸°í™”
        
        Args:
            rss_config: RSS í¬ë¡¤ë§ ì„¤ì • íŒŒì¼
            ai_config: AI ìƒì„± ì„¤ì • íŒŒì¼
            enable_ai: AI ì½˜í…ì¸  ìƒì„± í™œì„±í™” ì—¬ë¶€
        """
        self.enable_ai = enable_ai and AI_AVAILABLE
        
        # RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        self.news_automation = NewsAutomation(rss_config)
        
        # AI ìƒì„±ê¸° ì´ˆê¸°í™”
        if self.enable_ai:
            try:
                self.ai_generator = AIContentGenerator(ai_config)
                print("âœ… AI ì½˜í…ì¸  ìƒì„± í™œì„±í™”")
            except Exception as e:
                print(f"âš ï¸ AI ìƒì„±ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enable_ai = False
        else:
            self.ai_generator = None
            print("â„¹ï¸ AI ì½˜í…ì¸  ìƒì„± ë¹„í™œì„±í™” (RSSë§Œ ì‚¬ìš©)")
    
    def collect_rss_articles(self) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print("\n" + "="*60)
        print("ğŸ“° RSS ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        print("="*60)
        
        articles = self.news_automation.fetch_rss_feeds()
        
        if not articles:
            print("âŒ ìˆ˜ì§‘ëœ RSS ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"\nâœ… ì´ {len(articles)}ê°œ RSS ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # AI ìš”ì•½ ì ìš©
        processed = []
        for article in articles[:self.news_automation.config.get('max_articles', 20)]:
            if self.news_automation.config.get('use_ai_summary', False):
                article = self.news_automation.summarize_with_ai(article)
            else:
                article['summary'] = self.news_automation._clean_html(
                    article['summary']
                )[:200] + "..."
            
            processed.append({
                'title': article['title'],
                'source': article['source'],
                'time': self.news_automation.calculate_time_ago(
                    article.get('published', '')
                ),
                'summary': article['summary'],
                'link': article['link'],
                'image': article['image'],
                'category': 'AI/í…Œí¬',  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
                'type': 'rss'
            })
        
        return processed
    
    def generate_ai_article(self) -> Dict:
        """AIë¡œ ì½˜í…ì¸  ìë™ ìƒì„± ë° Markdown íŒŒì¼ ì €ì¥"""
        if not self.enable_ai:
            return None
        
        print("\n" + "="*60)
        print("ğŸ¤– AI ì½˜í…ì¸  ìƒì„± ì‹œì‘")
        print("="*60)
        
        try:
            article = self.ai_generator.create_article_for_blog()
            if article:
                article['type'] = 'ai_generated'
                
                # Markdown íŒŒì¼ë¡œ ì €ì¥
                self._save_ai_article_as_markdown(article)
                
                return article
        except Exception as e:
            print(f"âŒ AI ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return None
    
    def _save_ai_article_as_markdown(self, article: Dict):
        """AI ìƒì„± ê¸€ì„ Markdown íŒŒì¼ë¡œ ì €ì¥"""
        import os
        from datetime import datetime
        import re
        
        # contents ë””ë ‰í† ë¦¬ í™•ì¸ (ì ˆëŒ€ ê²½ë¡œ)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        contents_dir = os.path.join(project_root, 'contents')
        
        if not os.path.exists(contents_dir):
            os.makedirs(contents_dir)
        
        # íŒŒì¼ëª… ìƒì„± (ë‚ ì§œ-slug í˜•ì‹)
        today = datetime.now().strftime('%Y-%m-%d')
        
        # ì˜ë¬¸ slug ìƒì„±
        title = article['title']
        
        # 1. íŠ¹ìˆ˜ë¬¸ìë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ë³€í™˜
        title = title.replace('/', '-').replace(':', '-').replace('(', '').replace(')', '')
        
        # 2. ì˜ë¬¸, ìˆ«ì, ê³µë°±, í•˜ì´í”ˆë§Œ ë‚¨ê¸°ê³  ì œê±°
        title_slug = re.sub(r'[^a-zA-Z0-9\s-]', '', title)
        
        # 3. ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ì˜ í•˜ì´í”ˆìœ¼ë¡œ
        title_slug = re.sub(r'\s+', '-', title_slug.strip())
        
        # 4. ì—¬ëŸ¬ í•˜ì´í”ˆì„ í•˜ë‚˜ë¡œ
        title_slug = re.sub(r'-+', '-', title_slug)
        
        # 5. ì•ë’¤ í•˜ì´í”ˆ ì œê±°
        title_slug = title_slug.strip('-').lower()
        
        # slugê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ëŒ€ì²´ slug ìƒì„±
        if not title_slug or len(title_slug) < 5:
            # ì¹´í…Œê³ ë¦¬ + íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ slug
            category = article.get('category', 'ai')
            timestamp = datetime.now().strftime('%H%M%S')
            title_slug = f"{category}-article-{timestamp}"
        
        # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (ìµœëŒ€ 50ì)
        title_slug = title_slug[:50].rstrip('-')
        
        filename = f"{today}-{title_slug}.md"
        filepath = os.path.join(contents_dir, filename)
        
        # Front Matter ìƒì„±
        front_matter = f"""---
title: "{article['title']}"
date: {article.get('created_at', datetime.now().strftime('%Y-%m-%d %H:%M'))}
category: ai
source: "AI/í…Œí¬"
summary: "{article.get('summary', '')[:200]}"
image: {article.get('image', '')}
tags: [AI, ìë™í™”, ìƒì‚°ì„±]
type: ai_generated
---

"""
        
        # Markdown ë‚´ìš©
        content = article.get('content', '')
        
        # íŒŒì¼ ì €ì¥
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(front_matter)
                f.write(content)
            
            print(f"  âœ… Markdown íŒŒì¼ ì €ì¥: {filename}")
            
            # ë§í¬ ì—…ë°ì´íŠ¸
            article['link'] = f"/article.html?slug={today}-{title_slug}"
            
        except Exception as e:
            print(f"  âš ï¸ Markdown ì €ì¥ ì‹¤íŒ¨: {e}")
            article['link'] = "#"
    
    def merge_articles(self, 
                      rss_articles: List[Dict], 
                      ai_article: Dict = None) -> List[Dict]:
        """RSS ë‰´ìŠ¤ì™€ AI ìƒì„± ê¸€ í†µí•©"""
        all_articles = []
        
        # AI ìƒì„± ê¸€ì„ ë§¨ ì•ì— ì¶”ê°€
        if ai_article:
            print(f"\nğŸ¯ AI ìƒì„± ê¸€ ì¶”ê°€: {ai_article['title']}")
            all_articles.append(ai_article)
        
        # RSS ë‰´ìŠ¤ ì¶”ê°€
        all_articles.extend(rss_articles)
        
        print(f"\nğŸ“Š ì´ {len(all_articles)}ê°œ ì•„í‹°í´ (AI: {1 if ai_article else 0}, RSS: {len(rss_articles)})")
        
        return all_articles
    
    def load_existing_articles(self, data_file='data.json') -> List[Dict]:
        """ê¸°ì¡´ data.jsonì—ì„œ ê¸°ì‚¬ ë¡œë“œ"""
        try:
            if os.path.exists(data_file):
                with open(data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get('articles', [])
        except Exception as e:
            print(f"  âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []
    
    def archive_old_articles(self, articles: List[Dict], threshold: int = 50):
        """
        ì˜¤ë˜ëœ ê¸€ì„ ì•„ì¹´ì´ë¸Œ íŒŒì¼ë¡œ ì´ë™
        
        Args:
            articles: ì „ì²´ ê¸°ì‚¬ ëª©ë¡
            threshold: ë©”ì¸ í˜ì´ì§€ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        
        Returns:
            (ë©”ì¸ ê¸°ì‚¬ ëª©ë¡, ì•„ì¹´ì´ë¸Œëœ ê¸°ì‚¬ ìˆ˜)
        """
        if len(articles) <= threshold:
            return articles, 0
        
        # ë©”ì¸: ìµœì‹  50ê°œ
        main_articles = articles[:threshold]
        
        # ì•„ì¹´ì´ë¸Œ: 51ë²ˆì§¸ë¶€í„°
        archive_articles = articles[threshold:]
        
        # ì•„ì¹´ì´ë¸Œ íŒŒì¼ ë¡œë“œ (ê¸°ì¡´ ì•„ì¹´ì´ë¸Œ + ìƒˆ ì•„ì¹´ì´ë¸Œ)
        archive_path = 'archive.json'
        existing_archive = []
        
        try:
            if os.path.exists(archive_path):
                with open(archive_path, 'r', encoding='utf-8') as f:
                    archive_data = json.load(f)
                    existing_archive = archive_data.get('articles', [])
        except Exception as e:
            print(f"  âš ï¸ ì•„ì¹´ì´ë¸Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ì•„ì¹´ì´ë¸Œì— ì¶”ê°€
        archive_titles = {a['title'] for a in existing_archive}
        new_archived = 0
        
        for article in archive_articles:
            if article['title'] not in archive_titles:
                existing_archive.insert(0, article)  # ìµœì‹  ìˆœ ìœ ì§€
                new_archived += 1
        
        # ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì €ì¥
        if new_archived > 0 or not os.path.exists(archive_path):
            archive_data = {
                'updatedAt': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'totalArticles': len(existing_archive),
                'articles': existing_archive
            }
            
            with open(archive_path, 'w', encoding='utf-8') as f:
                json.dump(archive_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“¦ ì•„ì¹´ì´ë¸Œ: {new_archived}ê°œ ìƒˆë¡œ ì¶”ê°€, ì´ {len(existing_archive)}ê°œ ë³´ê´€")
        
        return main_articles, len(archive_articles)
    
    def create_data_json(self, articles: List[Dict], max_articles: int = 50) -> Dict:
        """
        data.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ í¬í•¨)
        
        Args:
            articles: ìƒˆë¡œ ì¶”ê°€í•  ê¸°ì‚¬ ëª©ë¡
            max_articles: ë©”ì¸ í˜ì´ì§€ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 50ê°œ)
        
        Notes:
            - ë©”ì¸: ìµœì‹  50ê°œ (ë¹ ë¥¸ ë¡œë”©)
            - ì•„ì¹´ì´ë¸Œ: 51ê°œë¶€í„° ëª¨ë‘ ë³´ê´€ (archive.json)
            - ëª¨ë“  ê¸€ ì˜êµ¬ ë³´ì¡´
        """
        # 1. ê¸°ì¡´ ê¸°ì‚¬ ë¡œë“œ
        existing = self.load_existing_articles()
        print(f"\nğŸ“š ê¸°ì¡´ ë©”ì¸ ê¸°ì‚¬: {len(existing)}ê°œ")
        
        # 2. ìƒˆ ê¸°ì‚¬ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        existing_titles = {article['title'] for article in existing}
        new_count = 0
        
        for article in articles:
            if article['title'] not in existing_titles:
                existing.insert(0, article)  # ìµœì‹  ê¸€ì„ ë§¨ ì•ì— ì¶”ê°€
                new_count += 1
        
        print(f"â• ì‹ ê·œ ê¸°ì‚¬: {new_count}ê°œ ì¶”ê°€")
        
        # 3. ì•„ì¹´ì´ë¸Œ ì²˜ë¦¬ (50ê°œ ì´ˆê³¼ ì‹œ)
        main_articles, archived_count = self.archive_old_articles(existing, max_articles)
        
        if archived_count > 0:
            print(f"ğŸ“¦ ì•„ì¹´ì´ë¸Œë¡œ ì´ë™: {archived_count}ê°œ")
        
        print(f"ğŸ“Š ë©”ì¸ í˜ì´ì§€: {len(main_articles)}ê°œ (ë¡œë”© ìµœì í™”)")
        
        return {
            'updatedAt': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'totalArticles': len(main_articles),
            'hasArchive': archived_count > 0,
            'articles': main_articles
        }
    
    def save_data_json(self, data: Dict, output_path='data.json'):
        """data.json ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ {output_path} ì €ì¥ ì™„ë£Œ!")
        print(f"   - ì´ {len(data['articles'])}ê°œ ì•„í‹°í´")
        print(f"   - ì—…ë°ì´íŠ¸: {data['updatedAt']}")
    
    def run(self, include_ai=True):
        """ì „ì²´ ìë™í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸš€ í†µí•© ë¸”ë¡œê·¸ ìë™í™” ì‹œì‘")
        print("="*60)
        print(f"ëª¨ë“œ: {'RSS + AI ìƒì„±' if include_ai and self.enable_ai else 'RSSë§Œ'}")
        
        # 1. RSS ë‰´ìŠ¤ ìˆ˜ì§‘
        rss_articles = self.collect_rss_articles()
        
        # 2. AI ì½˜í…ì¸  ìƒì„± (ì˜µì…˜)
        ai_article = None
        if include_ai and self.enable_ai:
            ai_article = self.generate_ai_article()
        
        # 3. í†µí•©
        all_articles = self.merge_articles(rss_articles, ai_article)
        
        if not all_articles:
            print("\nâŒ ìƒì„±ëœ ì•„í‹°í´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 4. data.json ìƒì„± ë° ì €ì¥
        data = self.create_data_json(all_articles)
        self.save_data_json(data)
        
        print("\n" + "="*60)
        print("ğŸ‰ ìë™í™” ì™„ë£Œ!")
        print("="*60)
        
        # ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“° ìƒì„±ëœ ì•„í‹°í´ ëª©ë¡:")
        for i, article in enumerate(all_articles[:5], 1):
            article_type = "ğŸ¤– AI" if article.get('type') == 'ai_generated' else "ğŸ“¡ RSS"
            print(f"{i}. {article_type} | {article['title'][:50]}...")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í†µí•© ë¸”ë¡œê·¸ ìë™í™”')
    parser.add_argument('--rss-config', default='config.json', help='RSS ì„¤ì • íŒŒì¼')
    parser.add_argument('--ai-config', default='config_ai.json', help='AI ì„¤ì • íŒŒì¼')
    parser.add_argument('--no-ai', action='store_true', help='AI ìƒì„± ë¹„í™œì„±í™”')
    parser.add_argument('--ai-only', action='store_true', help='AI ìƒì„±ë§Œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    try:
        automation = BlogAutomation(
            rss_config=args.rss_config,
            ai_config=args.ai_config,
            enable_ai=not args.no_ai
        )
        
        automation.run(include_ai=not args.no_ai)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
